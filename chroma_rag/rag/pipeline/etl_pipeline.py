from copy import deepcopy
from itertools import islice
import logging
from typing import Any, Callable, Iterable

import chromadb
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings.huggingface_hub import HuggingFaceHubEmbeddings
from langchain_community.vectorstores import utils as chromautils
from langchain_community.vectorstores.chroma import Chroma
from langchain_core.documents import BaseDocumentTransformer
from langchain_core.documents import Document
from pydantic_settings import BaseSettings

from chroma_rag.rag.pipeline.docs_processing.exceptions import PathIsNotAssigned
from chroma_rag.rag.pipeline.docs_processing.utils import get_loader
from chroma_rag.rag.settings.settings import settings as default_settings


logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)


class DocsExtractPipeline:
    """This class handles the initial loading of documents from a directory or file, and it can transition
    to the next step in the pipeline.
    """

    def __init__(self, pipeline_settings: "PipelineSettings"):
        self._config_dict = pipeline_settings.config_structure
        self._pipeline_settings = pipeline_settings

    def load_docs(self, docs_collection_path: str | None = None):
        """Loads documents from the specified path or from the default path defined in the configuration.

        This method initializes the document parsing process. If a document path is not provided
        and the configuration does not contain a valid path, it raises an error.

        Args:
            docs_collection_path (str | None): Optional path to the document collection. If not provided,
                the path from the configuration will be used.

        Returns:
            generator: A generator yielding documents loaded by the appropriate loader.

        Raises:
            PathIsNotAssigned: If no document path is provided or configured.
        """
        logger.info("Initialize parsing process")
        if docs_collection_path is None and self._config_dict.loader.doc_path == "":
            raise PathIsNotAssigned("Input file (directory) path is not assigned")
        if docs_collection_path is None:
            load_path = self._config_dict.loader.doc_path
        else:
            load_path = docs_collection_path

        parser_params = deepcopy(self._config_dict.loader.parsing_params)
        parser_params["file_path"] = load_path

        loader = get_loader(load_path, parser_params)
        return loader.lazy_load()

    def go_to_next_step(
        self, docs_collection_path: str | None = None
    ) -> "DocsTransformPipeline":
        # Goes to the next step in the document processing pipeline.
        return DocsTransformPipeline(
            self._pipeline_settings, self.load_docs(docs_collection_path)
        )


class DocsTransformPipeline:
    """Pipeline for transforming documents using a series of document transformers.

    This class is responsible for applying transformations to documents generated by a document extractor.
    The transformations are applied in sequence by a list of transformers.
    """

    def __init__(
        self,
        pipeline_settings: "PipelineSettings",
        docs_generator: Iterable[Document],
    ):
        self._docs_generator: Iterable[Document] = docs_generator
        self._transformers: list[BaseDocumentTransformer] = pipeline_settings.transformers

        def transformation(docs: list[Document]) -> list[Document]:
            # Applies all transformations in sequence to a list of documents.
            for transformer in self._transformers:
                docs = transformer.transform_documents(docs)
            return docs

        self._transformation: Callable[[list[Document]], list[Document]] = transformation

    def update_docs_transformers(
        self, **kwargs: dict[str, Any]
    ) -> "DocsTransformPipeline":
        """Updates the attributes of the document transformers with the provided keyword arguments.

        This method allows for updating the configuration of transformers dynamically. If any of the
        provided keyword arguments match an attribute in the transformer, that attribute is updated.

        Args:
            kwargs (dict[str, Any]): A dictionary of attribute names and values to update in the transformers.

        Returns:
            DocsTransformPipeline: The current pipeline instance with updated transformers.
        """
        if self._transformers is not None:
            if kwargs:
                for transformer in self._transformers:
                    for key, value in kwargs.items():
                        if key in transformer.__dict__:
                            transformer.__dict__[key] = value
                        if f"_{key}" in transformer.__dict__:
                            transformer.__dict__[f"_{key}"] = value

            def transformation(docs: list[Document]) -> list[Document]:
                for transformer in self._transformers:
                    docs = transformer.transform_documents(docs)
                return docs

            self._transformation = transformation
        return self

    def transform(self, batch_size: int | None = None) -> Iterable[list[Document]]:
        """Transforms documents in batches using the configured transformers.

        This method applies transformations to the documents generated by the `docs_generator` in batches.
        The batch size can be specified, and the method will yield each transformed batch.

        Args:
            batch_size (int | None): The size of the batches in which documents will be transformed.
                Defaults to 1 if not provided.

        Yields:
            Iterable[list[Document]]: An iterable of lists containing transformed documents.
        """
        if batch_size is None:
            batch_size = 1
        batch_size = max(batch_size, 1)
        while batch := list(islice(self._docs_generator, batch_size)):
            docs = self._transformation(batch)
            yield docs

    def go_to_next_step(self, batch_size: int | None = None) -> "DocsLoadPipeline":
        logger.info("Initialize transformation process")
        return DocsLoadPipeline(self.transform(batch_size))


class DocsLoadPipeline:
    """Pipeline for loading transformed documents into a storage backend.

    This class is responsible for loading documents into a Chroma DB collection after they have been transformed.
    It uses a generator to process and store documents in batches.
    """

    def __init__(self, docs_generator: Iterable[list[Document]]):
        self._docs_generator: Iterable[list[Document]] = docs_generator
        self._store_settings: BaseSettings = default_settings

    def store_settings(self, settings: BaseSettings | None = None) -> "DocsLoadPipeline":
        # Updates the storage settings for the pipeline.
        self._store_settings: BaseSettings = settings
        return self

    def load(
        self,
        loading_batch_size: int | None = None,
        collection_name: str | None = None,
    ) -> None:
        """Loads documents into the ChromaDB backend in batches.

        Args:
            loading_batch_size (int | None): The size of the batches in which documents will be loaded.
                Defaults to 32 if not provided.
            collection_name (str | None): The name of the collection in the Chroma DB where documents
                will be stored. If not provided, the default collection name from the settings is used.

        Returns:
            None
        """
        logger.info("Initialize loading process")
        if loading_batch_size is None:
            loading_batch_size = 32
        if self._store_settings.embedding_host:
            embedding_function = HuggingFaceHubEmbeddings(
                model=self._store_settings.embedding_host
            )
        else:
            embedding_function = HuggingFaceEmbeddings(
                model_name=self._store_settings.embedding_name
            )

        # Creating Chroma DB Client
        chroma_client = chromadb.HttpClient(
            host=self._store_settings.chroma_host,
            port=self._store_settings.chroma_port,
            settings=chromadb.Settings(allow_reset=self._store_settings.allow_reset),
        )

        if collection_name is None:
            collection_name = self._store_settings.collection_name
        else:
            collection_name = collection_name
        chroma_store = Chroma(
            collection_name=collection_name,
            embedding_function=embedding_function,
            client=chroma_client,
            collection_metadata={"hnsw:space": self._store_settings.distance_fn},
        )

        # Add processed documents to the defined Chroma collection
        loading_batch_size = max(loading_batch_size, 1)
        for docs_batch in self._docs_generator:
            if isinstance(docs_batch, list):
                loading_batches = [
                    docs_batch[i : i + loading_batch_size]
                    for i in range(0, len(docs_batch), loading_batch_size)
                ]
            else:
                loading_batches = [chromautils.filter_complex_metadata([docs_batch])]
            for batch in loading_batches:
                chroma_store.add_documents(batch)
