{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import geojson\n",
    "from typing import Any, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (AutoTokenizer,\n",
    "                          pipeline,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoConfig, GenerationConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('').resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(ROOT / 'config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct' #\"meta-llama/Meta-Llama-3-8B\"\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "SAVE_FOLDER = Path(ROOT / 'llama_answers_dataset_en_instruction').resolve()\n",
    "SAVE_FOLDER.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_DATA = ['building_area', 'living_area', 'storeys_count', 'resident_number', 'population_balanced', 'lift_count', 'building_year']\n",
    "\n",
    "\n",
    "def preprocess_context(data: Dict) -> Dict:\n",
    "    \"\"\"Preprocess context data by rounding all digits in manually set fields.\"\"\"\n",
    "    preprocessed = deepcopy(data)\n",
    "    for chunk in preprocessed['features']:\n",
    "        properties = chunk['properties']\n",
    "        for num_col in NUMERICAL_DATA:\n",
    "            if properties.get(num_col) is not None:\n",
    "                properties[num_col] = round(float(properties[num_col]))\n",
    "    return preprocessed\n",
    "        \n",
    "\n",
    "\n",
    "def get_prompt(question: str, context: Dict, *args, **kwargs) -> str:\n",
    "    \"\"\"Function for intialization of LLAMA3 prompt template.\"\"\"\n",
    "    default = '''Your name is Larry, You are smart AI assistant, You have high experitce in field of city building, urbanistic and Structure of St. Petersburg.'''\n",
    "    default_rules = f'''Answer the question following rules below. For answer you must use provided by user context. Your answer should be in Russian language.\n",
    "    Rules:\n",
    "    1. The answer should have three sentences maximum.\n",
    "    2. Add a unit of measurement to an answer.\n",
    "    3. If there are several organizations in the building, all of them should be mentioned in the answer.\n",
    "    4. The building's address (street, house number, building) in the user's question should exactly match a building address from the context.\n",
    "    5. For answer you should take only that infromation from context, which exactly match a building address (street, house number, building) from the user's question.\n",
    "    6. If provided by user context for a given address has \"null\" or \"None\" for the property, it means the data about this property of the building is absent.\n",
    "    7. In questions about building failure, 0 in the context's corresponding field means \"no\", and 1 - means \"yes\".\n",
    "    8. If data for an answer is absent, answer that data was not provided or absent and mention for what field there was no data.\n",
    "    9. If you do not know how to answer the questions, say so.\n",
    "    10. Before give an answer to the user question, provide explanation. Mark the answer with keyword \"ANSWER\", and explanation with \"EXPLANATION\".\n",
    "    11. You must use only provided information for the answer.'''\n",
    "    system_prompt = kwargs.get('system_prompt', default)\n",
    "    rules = kwargs.get('additional_rules', default_rules)\n",
    "    template = f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            {system_prompt} {rules}<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Контекст :{context} Вопрос: {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return template\n",
    "\n",
    "\n",
    "def save_answer_as_json(answer: Dict) -> None:\n",
    "    total_files_in_folder = len(glob.glob1(str(SAVE_FOLDER), '*.json'))\n",
    "    f_pref = 'llama_ans'\n",
    "    with open((SAVE_FOLDER/f'{f_pref}_{total_files_in_folder + 1}.json').resolve(), 'w', encoding='utf-8') as pth:\n",
    "        json.dump(answer, pth)\n",
    "\n",
    "\n",
    "def get_query(idx: int) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load set of queries and context to them.\"\"\"\n",
    "    with open(Path(ROOT, 'data', 'datasets', f'data_{idx}.json')) as json_data:\n",
    "        questions = json.load(json_data)\n",
    "    with open(Path(ROOT, 'data', 'buildings', f'buildings_part_{idx}.geojson')) as buildings_data:\n",
    "        manual_context = geojson.load(buildings_data)\n",
    "    return questions, manual_context\n",
    "\n",
    "\n",
    "def multi_ans(model: Any, amount: int = 10, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Get multiple answers from given model.\n",
    "    This function loads contexts from several files\n",
    "    \"\"\"\n",
    "    generation_temperature = kwargs.get('temperature', .5)\n",
    "    for i in range(amount):\n",
    "        queries, context = get_query(i)\n",
    "        total_questions = list(queries.keys())\n",
    "        for q_id in range(len(total_questions) // 5):\n",
    "            # Pick one query from the list\n",
    "            question_response_pair: Dict= queries[total_questions[q_id]]            \n",
    "            query, response = question_response_pair['query'], question_response_pair['response']\n",
    "\n",
    "            #Form a prompt from query and context\n",
    "            prompt = get_prompt(question=query, context=preprocess_context(context))\n",
    "            answer = model(prompt, temperature=generation_temperature)\n",
    "\n",
    "            json_ans = {'query': query, \n",
    "                        'llama_answer': answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1], \n",
    "                        'ideal_ans': response,\n",
    "                        'chunk': i,\n",
    "                        'question_number': total_questions[q_id]}\n",
    "            save_answer_as_json(json_ans)\n",
    "    print(f'Answers have been saved to {SAVE_FOLDER}. Amount: {len(glob.glob1(str(SAVE_FOLDER), \"*.json\"))}')\n",
    "\n",
    "\n",
    "def get_question_context_ans_strategy(idx: int) -> Tuple[str]:\n",
    "    \"\"\"RAG imulation with csv dataset.\"\"\"\n",
    "    strategy_dataset = pd.read_csv(ROOT / 'data' / 'strategy_questions.csv')\n",
    "    strategy_dataset.rename(columns={'Примеры 29.05': 'Question', 'Unnamed: 1': 'Context', 'Unnamed: 2': 'Answer'}, inplace=True)\n",
    "    strategy_dataset.drop(0, inplace=True)\n",
    "    return strategy_dataset['Question'][idx+1], strategy_dataset['Context'][idx+1], strategy_dataset['Answer'][idx+1]\n",
    "\n",
    "\n",
    "def process_str(s: str) -> str:\n",
    "    \"\"\"Remove line braking symblos from the string.\"\"\"\n",
    "    trans_s = s.translate(str.maketrans('\\n\\t\\r', '   '))\n",
    "    return re.compile(r\"\\s+\").sub(\" \", trans_s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/urbanistic/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091cc1f8d5ac4337a6e916f4c2259b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "model_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, max_new_tokens=12000, force_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b33ca6561c4e85b8c054a58254bc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # quantization_config=quantization_config, ## Uncomment, if quantization is required\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "print('Model is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=12000,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI ans testing\n",
    "# multi_ans(pipeline, amount=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, manual_context = get_query(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question_id: 51_3\n",
      "Question: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n"
     ]
    }
   ],
   "source": [
    "# File in question has multiple varinats of questions, we have to pick one\n",
    "question_id = list(questions.keys())[25]\n",
    "question_response_pair = questions[question_id]\n",
    "manual_question = question_response_pair['query']\n",
    "target = question_response_pair['response']\n",
    "\n",
    "print(f'Question_id: {question_id}')\n",
    "print(f'Question: {manual_question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_prompt(question=manual_question, context=preprocess_context(manual_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n",
      "Верный ответ: Для дома по адресу \"Санкт-Петербург, Шепетовская, 3\" данная информация отсутствует\n",
      "Ответ: \n",
      "             EXPLANATION: The context provides information about several buildings in St. Petersburg, including their addresses, administrative units, and other characteristics. To answer the question, we need to find the building with the address \"Санкт-Петербург, Шепетовская, 3\".\n",
      "\n",
      "ANSWER: According to the context, the building with the address \"Санкт-Петербург, Шепетовская, 3\" has a project type of \"null\".\n"
     ]
    }
   ],
   "source": [
    "answer = pipeline(query, temperature=.2)\n",
    "\n",
    "print(f'Вопрос: {manual_question}')\n",
    "print(f'Верный ответ: {target}')\n",
    "print(f'Ответ: {answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Question: Какие проблемы демографического развития Санкт-Петербурга?\n",
      "● Expected response: Ключевыми проблемами демографического развития Санкт-Петербурга являются: высокий уровень смертности (в\n",
      "первую очередь у мужчин трудоспособного возраста), депопуляция коренного населения (на протяжении длительного\n",
      "периода времени уровень рождаемости не обеспечивал простое замещение родительского поколения поколением\n",
      "детей), старение населения, диспропорции в гендерном составе жителей (стабильное преобладание количества\n",
      "женщин над количеством мужчин).\n",
      "● Answer:  высокий уровень смертности (в первую очередь у мужчин трудоспособного возраста), депопуляция коренного населения, старение населения, диспропорции в гендерном составе жителей.\n",
      "● Explanation: EXPLANATION: В контексте задачи по реализации комплекса мер демографического развития Санкт-Петербурга, ключевые проблемы демографического развития города включают высокий уровень смертности, депопуляцию коренного населения, старение населения и диспропорции в гендерном составе жителей. \n"
     ]
    }
   ],
   "source": [
    "question, context, correct_ans = get_question_context_ans_strategy(0)\n",
    "question = process_str(question)\n",
    "context = process_str(context)\n",
    "query = get_prompt(question=question, context=context)\n",
    "answer = pipeline(query, temperature=.015)\n",
    "\n",
    "answer_edited = process_str(answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1])\n",
    "\n",
    "print(f'● Question: {question}')\n",
    "print(f'● Expected response: {correct_ans}')\n",
    "print(f'● Answer: {answer_edited.split(\"ANSWER:\")[-1]}')\n",
    "print(f'● Explanation: {answer_edited.split(\"ANSWER\")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanistic",
   "language": "python",
   "name": "urbanistic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
