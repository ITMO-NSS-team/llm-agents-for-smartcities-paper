{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import geojson\n",
    "from typing import Any, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (AutoTokenizer,\n",
    "                          pipeline,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoConfig, GenerationConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('').resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(ROOT / 'config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct' #\"meta-llama/Meta-Llama-3-8B\"\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "SAVE_FOLDER = Path(ROOT / 'llama_answers_dataset_en_instruction').resolve()\n",
    "SAVE_FOLDER.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_DATA = ['building_area', 'living_area', 'storeys_count', 'resident_number', 'population_balanced', 'lift_count', 'building_year']\n",
    "\n",
    "\n",
    "def preprocess_context(data: Dict) -> Dict:\n",
    "    \"\"\"Preprocess context data by rounding all digits in manually set fields.\"\"\"\n",
    "    preprocessed = deepcopy(data)\n",
    "    for chunk in preprocessed['features']:\n",
    "        properties = chunk['properties']\n",
    "        for num_col in NUMERICAL_DATA:\n",
    "            if properties.get(num_col) is not None:\n",
    "                properties[num_col] = round(float(properties[num_col]))\n",
    "    return preprocessed\n",
    "        \n",
    "\n",
    "\n",
    "def get_prompt(question: str, context: Dict, *args, **kwargs) -> str:\n",
    "    \"\"\"Function for intialization of LLAMA3 prompt template.\"\"\"\n",
    "    default = '''Your name is Larry, You are smart AI assistant, You have high experitce in field of city building, urbanistic and Structure of St. Petersburg.'''\n",
    "    default_rules = f'''Answer the question following rules below. For answer you must use provided by user context.\n",
    "    Rules:\n",
    "    1. The answer should have three sentences maximum.\n",
    "    2. Add a unit of measurement to an answer.\n",
    "    3. If there are several organizations in the building, all of them should be mentioned in the answer.\n",
    "    4. The building's address (street, house number, building) in the user's question should exactly match a building address from the context.\n",
    "    5. For answer you should take only that infromation from context, which exactly match a building address (street, house number, building) from the user's question.\n",
    "    6. If provided by user context for a given address has \"null\" or \"None\" for the property, it means the data about this property of the building is absent.\n",
    "    7. In questions about building failure, 0 in the context's corresponding field means \"no\", and 1 - means \"yes\".\n",
    "    8. If data for an answer is absent, answer that data was not provided or absent and mention for what field there was no data.\n",
    "    9. If you do not know how to answer the questions, say so.\n",
    "    10. Before give an answer to the user question, provide explanation. Mark the answer with keyword \"ANSWER\", and explanation with \"EXPLANATION\".\n",
    "    11. You must use only provided information for the answer.\n",
    "    12. Your answer should be in Russian language.'''\n",
    "    system_prompt = kwargs.get('system_prompt', default)\n",
    "    rules = kwargs.get('additional_rules', default_rules)\n",
    "    template = f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            {system_prompt} {rules}<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Контекст :{context} Вопрос: {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return template\n",
    "\n",
    "\n",
    "def save_answer_as_json(answer: Dict) -> None:\n",
    "    total_files_in_folder = len(glob.glob1(str(SAVE_FOLDER), '*.json'))\n",
    "    f_pref = 'llama_ans'\n",
    "    with open((SAVE_FOLDER/f'{f_pref}_{total_files_in_folder + 1}.json').resolve(), 'w', encoding='utf-8') as pth:\n",
    "        json.dump(answer, pth)\n",
    "\n",
    "\n",
    "def get_query(idx: int) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load set of queries and context to them.\"\"\"\n",
    "    with open(Path(ROOT, 'data', 'datasets', f'data_{idx}.json')) as json_data:\n",
    "        questions = json.load(json_data)\n",
    "    with open(Path(ROOT, 'data', 'buildings', f'buildings_part_{idx}.geojson')) as buildings_data:\n",
    "        manual_context = geojson.load(buildings_data)\n",
    "    return questions, manual_context\n",
    "\n",
    "\n",
    "def multi_ans(model: Any, amount: int = 10, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Get multiple answers from given model.\n",
    "    This function loads contexts from several files\n",
    "    \"\"\"\n",
    "    generation_temperature = kwargs.get('temperature', .5)\n",
    "    for i in range(amount):\n",
    "        queries, context = get_query(i)\n",
    "        total_questions = list(queries.keys())\n",
    "        for q_id in range(len(total_questions) // 5):\n",
    "            # Pick one query from the list\n",
    "            question_response_pair: Dict= queries[total_questions[q_id]]            \n",
    "            query, response = question_response_pair['query'], question_response_pair['response']\n",
    "\n",
    "            #Form a prompt from query and context\n",
    "            prompt = get_prompt(question=query, context=preprocess_context(context))\n",
    "            answer = model(prompt, temperature=generation_temperature)\n",
    "\n",
    "            json_ans = {'query': query, \n",
    "                        'llama_answer': answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1], \n",
    "                        'ideal_ans': response,\n",
    "                        'chunk': i,\n",
    "                        'question_number': total_questions[q_id]}\n",
    "            save_answer_as_json(json_ans)\n",
    "    print(f'Answers have been saved to {SAVE_FOLDER}. Amount: {len(glob.glob1(str(SAVE_FOLDER), \"*.json\"))}')\n",
    "\n",
    "\n",
    "def get_question_context_ans_strategy(idx: int) -> Tuple[str]:\n",
    "    \"\"\"RAG imulation with csv dataset.\"\"\"\n",
    "    strategy_dataset = pd.read_csv(ROOT / 'data' / 'strategy_questions.csv')\n",
    "    strategy_dataset.rename(columns={'Примеры 29.05': 'Question', 'Unnamed: 1': 'Context', 'Unnamed: 2': 'Answer'}, inplace=True)\n",
    "    strategy_dataset.drop(0, inplace=True)\n",
    "    return strategy_dataset['Question'][idx+1], strategy_dataset['Context'][idx+1], strategy_dataset['Answer'][idx+1]\n",
    "\n",
    "\n",
    "def process_str(s: str) -> str:\n",
    "    \"\"\"Remove line braking symblos from the string.\"\"\"\n",
    "    trans_s = s.translate(str.maketrans('\\n\\t\\r', '   '))\n",
    "    return re.compile(r\"\\s+\").sub(\" \", trans_s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/urbanistic/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16cd75fcfd049ceb19a5c843bb07bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "model_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, max_new_tokens=12000, force_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0651383d3ad4eaab4cb566fac8f42f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # quantization_config=quantization_config, ## Uncomment, if quantization is required\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "print('Model is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=12000,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI ans testing\n",
    "# multi_ans(pipeline, amount=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question_id: 51_3\n",
      "Question: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n"
     ]
    }
   ],
   "source": [
    "# File in question has multiple varinats of questions, we have to pick one\n",
    "questions, manual_context = get_query(5)\n",
    "\n",
    "question_id = list(questions.keys())[25]\n",
    "question_response_pair = questions[question_id]\n",
    "manual_question = process_str(question_response_pair['query'])\n",
    "target = process_str(question_response_pair['response'])\n",
    "\n",
    "print(f'Question_id: {question_id}')\n",
    "print(f'Question: {manual_question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_prompt(question=manual_question, context=preprocess_context(manual_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Question: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n",
      "● Estimated answer: Для дома по адресу \"Санкт-Петербург, Шепетовская, 3\" данная информация отсутствует\n",
      "● Answer: : Для дома по адресу \"Санкт-Петербург, Шепетовская, 3\" тип проекта застройки не указан, т.к. в контексте для этого поля указано \"null\".\n",
      "● Explanation: EXPLANATION: The context provides information about several buildings in St. Petersburg, including their addresses, administrative units, and other characteristics. To answer the question, we need to find the building with the address \"Санкт-Петербург, Шепетовская, 3\" and check its \"project_type\" property. \n"
     ]
    }
   ],
   "source": [
    "answer = pipeline(query, temperature=.2)\n",
    "answer_edited = process_str(answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1])\n",
    "\n",
    "print(f'● Question: {manual_question}')\n",
    "print(f'● Estimated answer: {target}')\n",
    "print(f'● Answer: {answer_edited.split(\"ANSWER\")[-1]}')\n",
    "print(f'● Explanation: {answer_edited.split(\"ANSWER\")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Question: Какая средняя обеспеченность дошкольными учреждениями?\n",
      "● Expected response: Средняя обеспеченность дошкольными учреждениями в конце 2012 года в целом по Санкт-Петербургу составила\n",
      "95,65% (от числа детей в возрасте от 1 до 6 лет), средний показатель обеспеченности общеобразовательными\n",
      "учреждениями - 102,74%. Дефицита потребности в общеобразовательных учреждениях в целом по городу не\n",
      "отмечается. Однако в районах активной жилой застройки наблюдается дисбаланс обеспеченности, как по детским\n",
      "садам, так и по школам. В Санкт-Петербурге сформирована высокоразвитая сфера дополнительного образования:\n",
      "более 85% детей охвачены программами дополнительного образования.\n",
      "● Answer:  Средняя обеспеченность дошкольными учреждениями в конце 2012 года в целом по Санкт-Петербургу составила 95,65% (от числа детей в возрасте от 1 до 6 лет).\n",
      "● Explanation: EXPLANATION: В контексте предоставленной информации мы находим информацию о средней обеспеченности дошкольными учреждениями в Санкт-Петербурге. \n"
     ]
    }
   ],
   "source": [
    "question, context, correct_ans = get_question_context_ans_strategy(0)\n",
    "question = process_str(question)\n",
    "context = process_str(context)\n",
    "query = get_prompt(question=question, context=context)\n",
    "answer = pipeline(query, temperature=.015)\n",
    "\n",
    "answer_edited = process_str(answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1])\n",
    "\n",
    "print(f'● Question: {question}')\n",
    "print(f'● Expected response: {correct_ans}')\n",
    "print(f'● Answer: {answer_edited.split(\"ANSWER:\")[-1]}')\n",
    "print(f'● Explanation: {answer_edited.split(\"ANSWER\")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanistic",
   "language": "python",
   "name": "urbanistic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
